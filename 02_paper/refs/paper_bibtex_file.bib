
@article{nethery_estimating_2019,
	title = {{ESTIMATING} {POPULATION} {AVERAGE} {CAUSAL} {EFFECTS} {IN} {THE} {PRESENCE} {OF} {NON}-{OVERLAP}: {THE} {EFFECT} {OF} {NATURAL} {GAS} {COMPRESSOR} {STATION} {EXPOSURE} {ON} {CANCER} {MORTALITY}},
	volume = {13},
	issn = {1932-6157},
	shorttitle = {{ESTIMATING} {POPULATION} {AVERAGE} {CAUSAL} {EFFECTS} {IN} {THE} {PRESENCE} {OF} {NON}-{OVERLAP}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6658123/},
	doi = {10.1214/18-AOAS1231},
	abstract = {Most causal inference studies rely on the assumption of overlap to estimate population or sample average causal effects. When data suffer from non-overlap, estimation of these estimands requires reliance on model specifications, due to poor data support. All existing methods to address non-overlap, such as trimming or down-weighting data in regions of poor data support, change the estimand so that inference cannot be made on the sample or the underlying population. In environmental health research settings, where study results are often intended to influence policy, population-level inference may be critical, and changes in the estimand can diminish the impact of the study results, because estimates may not be representative of effects in the population of interest to policymakers. Researchers may be willing to make additional, minimal modeling assumptions in order to preserve the ability to estimate population average causal effects. We seek to make two contributions on this topic. First, we propose a flexible, data-driven definition of propensity score overlap and non-overlap regions. Second, we develop a novel Bayesian framework to estimate population average causal effects with minor model dependence and appropriately large uncertainties in the presence of non-overlap and causal effect heterogeneity. In this approach, the tasks of estimating causal effects in the overlap and non-overlap regions are delegated to two distinct models, suited to the degree of data support in each region. Tree ensembles are used to non-parametrically estimate individual causal effects in the overlap region, where the data can speak for themselves. In the non-overlap region, where insufficient data support means reliance on model specification is necessary, individual causal effects are estimated by extrapolating trends from the overlap region via a spline model. The promising performance of our method is demonstrated in simulations. Finally, we utilize our method to perform a novel investigation of the causal effect of natural gas compressor station exposure on cancer outcomes. Code and data to implement the method and reproduce all simulations and analyses is available on Github (https://github.com/rachelnethery/overlap).},
	number = {2},
	urldate = {2023-09-06},
	journal = {The annals of applied statistics},
	author = {Nethery, Rachel C. and Mealli, Fabrizia and Dominici, Francesca},
	month = jun,
	year = {2019},
	pmid = {31346355},
	pmcid = {PMC6658123},
	pages = {1242--1267},
	file = {Nethery et al_2019_ESTIMATING POPULATION AVERAGE CAUSAL EFFECTS IN THE PRESENCE OF NON-OVERLAP.pdf:/Users/lennardmassmann/Zotero/storage/4Q6QVHKA/Nethery et al_2019_ESTIMATING POPULATION AVERAGE CAUSAL EFFECTS IN THE PRESENCE OF NON-OVERLAP.pdf:application/pdf},
}

@article{hill_assessing_2013,
	title = {Assessing lack of common support in causal inference using {Bayesian} nonparametrics: {Implications} for evaluating the effect of breastfeeding on children's cognitive outcomes},
	volume = {7},
	issn = {1932-6157},
	shorttitle = {Assessing lack of common support in causal inference using {Bayesian} nonparametrics},
	url = {http://arxiv.org/abs/1311.7244},
	doi = {10.1214/13-AOAS630},
	abstract = {Causal inference in observational studies typically requires making comparisons between groups that are dissimilar. For instance, researchers investigating the role of a prolonged duration of breastfeeding on child outcomes may be forced to make comparisons between women with substantially different characteristics on average. In the extreme there may exist neighborhoods of the covariate space where there are not sufficient numbers of both groups of women (those who breastfed for prolonged periods and those who did not) to make inferences about those women. This is referred to as lack of common support. Problems can arise when we try to estimate causal effects for units that lack common support, thus we may want to avoid inference for such units. If ignorability is satisfied with respect to a set of potential confounders, then identifying whether, or for which units, the common support assumption holds is an empirical question. However, in the high-dimensional covariate space often required to satisfy ignorability such identification may not be trivial. Existing methods used to address this problem often require reliance on parametric assumptions and most, if not all, ignore the information embedded in the response variable. We distinguish between the concepts of "common support" and common causal support." We propose a new approach for identifying common causal support that addresses some of the shortcomings of existing methods. We motivate and illustrate the approach using data from the National Longitudinal Survey of Youth to estimate the effect of breastfeeding at least nine months on reading and math achievement scores at age five or six. We also evaluate the comparative performance of this method in hypothetical examples and simulations where the true treatment effect is known.},
	number = {3},
	urldate = {2021-12-16},
	journal = {The Annals of Applied Statistics},
	author = {Hill, Jennifer and Su, Yu-Sung},
	month = sep,
	year = {2013},
	note = {arXiv: 1311.7244},
	keywords = {\_tablet, Statistics - Applications},
	file = {Hill_Su_2013_Assessing lack of common support in causal inference using Bayesian.pdf:/Users/lennardmassmann/Zotero/storage/F2CSFG5R/Hill_Su_2013_Assessing lack of common support in causal inference using Bayesian.pdf:application/pdf},
}


@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	language = {en},
	number = {1},
	urldate = {2021-03-30},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	keywords = {\_tablet},
	pages = {5--32},
	file = {Breiman_2001_Random Forests.pdf:/Users/lennardmassmann/Zotero/storage/4F5UNK6Y/Breiman_2001_Random Forests.pdf:application/pdf},
}

@Manual{R_programming,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2021},
    url = {https://www.R-project.org/},
   }

@article{sparapani_nonparametric_2021,
	title = {Nonparametric {Machine} {Learning} and {Efficient} {Computation} with {Bayesian} {Additive} {Regression} {Trees}: {The} {BART} {R} {Package}},
	volume = {97},
	copyright = {Copyright (c) 2021 Rodney Sparapani, Charles Spanbauer, Robert McCulloch},
	issn = {1548-7660},
	shorttitle = {Nonparametric {Machine} {Learning} and {Efficient} {Computation} with {Bayesian} {Additive} {Regression} {Trees}},
	url = {https://doi.org/10.18637/jss.v097.i01},
	doi = {10.18637/jss.v097.i01},
	abstract = {In this article, we introduce the BART R package which is an acronym for Bayesian additive regression trees. BART is a Bayesian nonparametric, machine learning, ensemble predictive modeling method for continuous, binary, categorical and time-to-event outcomes. Furthermore, BART is a tree-based, black-box method which fits the outcome to an arbitrary random function, f , of the covariates. The BART technique is relatively computationally efficient as compared to its competitors, but large sample sizes can be demanding. Therefore, the BART package includes efficient state-of-the-art implementations for continuous, binary, categorical and time-to-event outcomes that can take advantage of modern off-the-shelf hardware and software multi-threading technology. The BART package is written in C++ for both programmer and execution efficiency. The BART package takes advantage of multi-threading via forking as provided by the parallel package and OpenMP when available and supported by the platform. The ensemble of binary trees produced by a BART fit can be stored and re-used later via the R predict function. In addition to being an R package, the installed BART routines can be called directly from C++. The BART package provides the tools for your BART toolbox.},
	language = {en},
	urldate = {2023-12-18},
	journal = {Journal of Statistical Software},
	author = {Sparapani, Rodney and Spanbauer, Charles and McCulloch, Robert},
	month = jan,
	year = {2021},
	keywords = {binary trees, black-box, categorical, competing risks, continuous, ensemble predictive model, forking, multi-threading, multinomial, OpenMP, recurrent events, survival analysis},
	pages = {1--66},
}



@article{linero_bayesian_dart_2018,
	title = {Bayesian {Regression} {Trees} for {High}-{Dimensional} {Prediction} and {Variable} {Selection}},
	volume = {113},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2016.1264957},
	doi = {10.1080/01621459.2016.1264957},
	abstract = {Decision tree ensembles are an extremely popular tool for obtaining high-quality predictions in nonparametric regression problems. Unmodified, however, many commonly used decision tree ensemble methods do not adapt to sparsity in the regime in which the number of predictors is larger than the number of observations. A recent stream of research concerns the construction of decision tree ensembles that are motivated by a generative probabilistic model, the most influential method being the Bayesian additive regression trees (BART) framework. In this article, we take a Bayesian point of view on this problem and show how to construct priors on decision tree ensembles that are capable of adapting to sparsity in the predictors by placing a sparsity-inducing Dirichlet hyperprior on the splitting proportions of the regression tree prior. We characterize the asymptotic distribution of the number of predictors included in the model and show how this prior can be easily incorporated into existing Markov chain Monte Carlo schemes. We demonstrate that our approach yields useful posterior inclusion probabilities for each predictor and illustrate the usefulness of our approach relative to other decision tree ensemble approaches on both simulated and real datasets. Supplementary materials for this article are available online.},
	number = {522},
	urldate = {2023-10-12},
	journal = {Journal of the American Statistical Association},
	author = {Linero, Antonio R.},
	month = apr,
	year = {2018},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2016.1264957},
	keywords = {Random forests, Variable selection, Nonparametric regression, Bayesian additive regression trees, Bayesian learning, Decision trees},
	pages = {626--636},
	file = {Linero_2018_Bayesian Regression Trees for High-Dimensional Prediction and Variable Selection.pdf:/Users/lennardmassmann/Zotero/storage/CVAXRUX7/Linero_2018_Bayesian Regression Trees for High-Dimensional Prediction and Variable Selection.pdf:application/pdf},
}


@misc{rockova_posterior_2019,
	title = {Posterior {Concentration} for {Bayesian} {Regression} {Trees} and {Forests}},
	url = {http://arxiv.org/abs/1708.08734},
	abstract = {Since their inception in the 1980's, regression trees have been one of the more widely used non-parametric prediction methods. Tree-structured methods yield a histogram reconstruction of the regression surface, where the bins correspond to terminal nodes of recursive partitioning. Trees are powerful, yet susceptible to over-fitting. Strategies against overfitting have traditionally relied on pruning greedily grown trees. The Bayesian framework offers an alternative remedy against overfitting through priors. Roughly speaking, a good prior charges smaller trees where overfitting does not occur. While the consistency of random histograms, trees and their ensembles has been studied quite extensively, the theoretical understanding of the Bayesian counterparts has been missing. In this paper, we take a step towards understanding why/when do Bayesian trees and their ensembles not overfit. To address this question, we study the speed at which the posterior concentrates around the true smooth regression function. We propose a spike-and-tree variant of the popular Bayesian CART prior and establish new theoretical results showing that regression trees (and their ensembles) (a) are capable of recovering smooth regression surfaces, achieving optimal rates up to a log factor, (b) can adapt to the unknown level of smoothness and (c) can perform effective dimension reduction when p{\textgreater}n. These results provide a piece of missing theoretical evidence explaining why Bayesian trees (and additive variants thereof) have worked so well in practice.},
	language = {en},
	urldate = {2023-12-12},
	publisher = {arXiv},
	author = {Rockova, Veronika and van der Pas, Stephanie},
	month = jun,
	year = {2019},
	note = {arXiv:1708.08734 [math, stat]},
	keywords = {Mathematics - Statistics Theory},
	file = {Rockova und van der Pas - 2019 - Posterior Concentration for Bayesian Regression Tr.pdf:/Users/lennardmassmann/Zotero/storage/RRMFGBBF/Rockova und van der Pas - 2019 - Posterior Concentration for Bayesian Regression Tr.pdf:application/pdf},
}


@article{wendling_comparing_2018,
	title = {Comparing methods for estimation of heterogeneous treatment effects using observational data from health care databases},
	volume = {37},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7820},
	doi = {10.1002/sim.7820},
	abstract = {There is growing interest in using routinely collected data from health care databases to study the safety and effectiveness of therapies in “real-world” conditions, as it can provide complementary evidence to that of randomized controlled trials. Causal inference from health care databases is challenging because the data are typically noisy, high dimensional, and most importantly, observational. It requires methods that can estimate heterogeneous treatment effects while controlling for confounding in high dimensions. Bayesian additive regression trees, causal forests, causal boosting, and causal multivariate adaptive regression splines are off-the-shelf methods that have shown good performance for estimation of heterogeneous treatment effects in observational studies of continuous outcomes. However, it is not clear how these methods would perform in health care database studies where outcomes are often binary and rare and data structures are complex. In this study, we evaluate these methods in simulation studies that recapitulate key characteristics of comparative effectiveness studies. We focus on the conditional average effect of a binary treatment on a binary outcome using the conditional risk difference as an estimand. To emulate health care database studies, we propose a simulation design where real covariate and treatment assignment data are used and only outcomes are simulated based on nonparametric models of the real outcomes. We apply this design to 4 published observational studies that used records from 2 major health care databases in the United States. Our results suggest that Bayesian additive regression trees and causal boosting consistently provide low bias in conditional risk difference estimates in the context of health care database studies.},
	language = {en},
	number = {23},
	urldate = {2023-12-12},
	journal = {Statistics in Medicine},
	author = {Wendling, T. and Jung, K. and Callahan, A. and Schuler, A. and Shah, N. H. and Gallego, B.},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7820},
	keywords = {health care databases, heterogeneous treatment effects, machine learning, propensity score, simulation},
	pages = {3309--3324},
	file = {Full Text PDF:/Users/lennardmassmann/Zotero/storage/TZA58KCT/Wendling et al. - 2018 - Comparing methods for estimation of heterogeneous .pdf:application/pdf},
}


@article{hu_treebased_2020,
	title = {Tree‐{Based} {Machine} {Learning} to {Identify} and {Understand} {Major} {Determinants} for {Stroke} at the {Neighborhood} {Level}},
	volume = {9},
	issn = {2047-9980},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7763737/},
	doi = {10.1161/JAHA.120.016745},
	abstract = {Background
Stroke is a major cardiovascular disease that causes significant health and economic burden in the United States. Neighborhood community‐based interventions have been shown to be both effective and cost‐effective in preventing cardiovascular disease. There is a dearth of robust studies identifying the key determinants of cardiovascular disease and the underlying effect mechanisms at the neighborhood level. We aim to contribute to the evidence base for neighborhood cardiovascular health research.

Methods and Results
We created a new neighborhood health data set at the census tract level by integrating 4 types of potential predictors, including unhealthy behaviors, prevention measures, sociodemographic factors, and environmental measures from multiple data sources. We used 4 tree‐based machine learning techniques to identify the most critical neighborhood‐level factors in predicting the neighborhood‐level prevalence of stroke, and compared their predictive performance for variable selection. We further quantified the effects of the identified determinants on stroke prevalence using a Bayesian linear regression model. Of the 5 most important predictors identified by our method, higher prevalence of low physical activity, larger share of older adults, higher percentage of non‐Hispanic Black people, and higher ozone levels were associated with higher prevalence of stroke at the neighborhood level. Higher median household income was linked to lower prevalence. The most important interaction term showed an exacerbated adverse effect of aging and low physical activity on the neighborhood‐level prevalence of stroke.

Conclusions
Tree‐based machine learning provides insights into underlying drivers of neighborhood cardiovascular health by discovering the most important determinants from a wide range of factors in an agnostic, data‐driven, and reproducible way. The identified major determinants and the interactive mechanism can be used to prioritize and allocate resources to optimize community‐level interventions for stroke prevention.},
	number = {22},
	urldate = {2023-12-12},
	journal = {Journal of the American Heart Association: Cardiovascular and Cerebrovascular Disease},
	author = {Hu, Liangyuan and Liu, Bian and Ji, Jiayi and Li, Yan},
	month = nov,
	year = {2020},
	pmid = {33140687},
	pmcid = {PMC7763737},
	pages = {e016745},
	file = {PubMed Central Full Text PDF:/Users/lennardmassmann/Zotero/storage/B89NLIQ3/Hu et al. - 2020 - Tree‐Based Machine Learning to Identify and Unders.pdf:application/pdf},
}



@article{hill_bayesian_2020,
	title = {Bayesian {Additive} {Regression} {Trees}: {A} {Review} and {Look} {Forward}},
	volume = {7},
	issn = {2326-8298, 2326-831X},
	shorttitle = {Bayesian {Additive} {Regression} {Trees}},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-031219-041110},
	doi = {10.1146/annurev-statistics-031219-041110},
	abstract = {Bayesian additive regression trees (BART) provides a flexible approach to fitting a variety of regression models while avoiding strong parametric assumptions. The sum-of-trees model is embedded in a Bayesian inferential framework to support uncertainty quantification and provide a principled approach to regularization through prior specification. This article presents the basic approach and discusses further development of the original algorithm that support a variety of data structures and assumptions. We describe augmentations of the prior specification to accommodate higher dimensional data and smoother functions. Recent theoretical developments provide justifications for the performance observed in simulations and other settings. Use of BART in causal inference provides an additional avenue for extensions and applications. We discuss software options as well as challenges and future directions.},
	language = {en},
	number = {1},
	urldate = {2021-06-12},
	journal = {Annual Review of Statistics and Its Application},
	author = {Hill, Jennifer and Linero, Antonio and Murray, Jared},
	month = mar,
	year = {2020},
	keywords = {\_tablet},
	pages = {251--278},
	file = {Hill et al_2020_Bayesian Additive Regression Trees.pdf:/Users/lennardmassmann/Zotero/storage/TFZ4W5XP/Hill et al_2020_Bayesian Additive Regression Trees.pdf:application/pdf},
}

@article{dorie_automated_2019,
	title = {Automated versus {Do}-{It}-{Yourself} {Methods} for {Causal} {Inference}: {Lessons} {Learned} from a {Data} {Analysis} {Competition}},
	volume = {34},
	issn = {0883-4237},
	shorttitle = {Automated versus {Do}-{It}-{Yourself} {Methods} for {Causal} {Inference}},
	url = {https://www.jstor.org/stable/26771031},
	abstract = {Statisticians have made great progress in creating methods that reduce our reliance on parametric assumptions. However, this explosion in research has resulted in a breadth of inferential strategies that both create opportunities for more reliable inference as well as complicate the choices that an applied researcher has to make and defend. Relatedly, researchers advocating for new methods typically compare their method to at best 2 or 3 other causal inference strategies and test using simulations that may or may not be designed to equally tease out flaws in all the competing methods. The causal inference data analysis challenge, "Is Your SATT Where It's At?", launched as part of the 2016 Atlantic Causal Inference Conference, sought to make progress with respect to both of these issues. The researchers creating the data testing grounds were distinct from the researchers submitting methods whose efficacy would be evaluated. Results from 30 competitors across the two versions of the competition (black-box algorithms and do-it-yourself analyses) are presented along with post-hoc analyses that reveal information about the characteristics of causal inference strategies and settings that affect performance. The most consistent conclusion was that methods that flexibly model the response surface perform better overall than methods that fail to do so. Finally new methods are proposed that combine features of several of the top-performing submitted methods.},
	number = {1},
	urldate = {2023-12-12},
	journal = {Statistical Science},
	author = {Dorie, Vincent and Hill, Jennifer and Shalit, Uri and Scott, Marc and Cervone, Dan},
	year = {2019},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {43--68},
	file = {JSTOR Full Text PDF:/Users/lennardmassmann/Zotero/storage/47NWVKWP/Dorie et al. - 2019 - Automated versus Do-It-Yourself Methods for Causal.pdf:application/pdf},
}


@book{james_introduction_2021,
	address = {New York, NY},
	edition = {Second edition},
	series = {Springer texts in statistics},
	title = {An introduction to statistical learning : with applications in {R}},
	isbn = {978-1-07-161417-4 1-07-161417-7 978-1-07-161420-4 1-07-161420-7},
	language = {eng},
	publisher = {Springer New York, NY},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2021},
	note = {Section: xv, 607 pages : illustrations (chiefly color) ; 24 cm.},
}


@article{breiman_bagging_1996,
	title = {Bagging predictors},
	volume = {24},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00058655},
	doi = {10.1007/BF00058655},
	abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
	language = {en},
	number = {2},
	urldate = {2023-12-12},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = aug,
	year = {1996},
	keywords = {Aggregation, Averaging, Bootstrap, Combining},
	pages = {123--140},
	file = {Full Text PDF:/Users/lennardmassmann/Zotero/storage/PXPCS5XT/Breiman - 1996 - Bagging predictors.pdf:application/pdf},
}




@article{friedman_greedy_2001,
	title = {Greedy function approximation: {A} gradient boosting machine.},
	volume = {29},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Greedy function approximation},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full},
	doi = {10.1214/aos/1013203451},
	abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent “boosting” paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such “TreeBoost” models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
	number = {5},
	urldate = {2023-12-12},
	journal = {The Annals of Statistics},
	author = {Friedman, Jerome H.},
	month = oct,
	year = {2001},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62-02, 62-07, 62-08, 62G08, 62H30, 68T10, boosting, decision trees, Function estimation, robust nonparametric regression},
	pages = {1189--1232},
	file = {Full Text PDF:/Users/lennardmassmann/Zotero/storage/9DY5LWQT/Friedman - 2001 - Greedy function approximation A gradient boosting.pdf:application/pdf},
}


@article{chipman_bayesian_1998,
	title = {Bayesian {CART} {Model} {Search}},
	volume = {93},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2669832},
	doi = {10.2307/2669832},
	abstract = {In this article we put forward a Bayesian approach for finding classification and regression tree (CART) models. The two basic components of this approach consist of prior specification and stochastic search. The basic idea is to have the prior induce a posterior distribution that will guide the stochastic search toward more promising CART models. As the search proceeds, such models can then be selected with a variety of criteria, such as posterior probability, marginal likelihood, residual sum of squares or misclassification rates. Examples are used to illustrate the potential superiority of this approach over alternative methods.},
	number = {443},
	urldate = {2023-12-12},
	journal = {Journal of the American Statistical Association},
	author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
	year = {1998},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {935--948},
	file = {JSTOR Full Text PDF:/Users/lennardmassmann/Zotero/storage/E7GXUPJU/Chipman et al. - 1998 - Bayesian CART Model Search.pdf:application/pdf},
}



@article{visconti_handling_2018,
	title = {Handling {Limited} {Overlap} in {Observational} {Studies} with {Cardinality} {Matching}},
	volume = {4},
	issn = {2767-3324},
	url = {https://muse.jhu.edu/pub/56/article/793377},
	abstract = {A common problem encountered in observational studies is limited overlap in covariate distributions across treatment groups. To address this problem, and avoid strong modeling assumptions, it has become common practice to restrict analyses to the portions of the treatment groups that overlap or, ultimately, are balanced in their covariate distributions. Often, this is done by matching on the estimated propensity score or coarsened versions of the observed covariates. A recent alternative methodology that, in a sense, encompasses these two approaches is cardinality matching. Cardinality matching is a flexible matching method that uses integer programming to find the largest matched sample that is balanced according to criteria specified before matching by the investigator. In this paper, we apply and illustrate the method of cardinality matching and show how to use it to directly balance several features of the covariates, including their trajectories in time and their distributions, without requiring exact matching. We demonstrate how cardinality matching addresses the problem of limited overlap using the original covariates, as opposed to a summarized or coarsened version of them. We discuss how this method can be extended to build matched samples that are not only balanced but also representative of a target population by design. We also show how this method enhances sensitivity analyses for hidden biases. We explain these advancements through an observational study of the electoral impact of the 2010 earthquake in Chile.},
	number = {1},
	urldate = {2023-11-30},
	journal = {Observational Studies},
	author = {Visconti, Giancarlo and Zubizarreta, José R.},
	year = {2018},
	note = {Publisher: University of Pennsylvania Press},
	pages = {217--249},
	file = {Handling Limited Overlap in Observational Studies with Cardinality Matching:/Users/lennardmassmann/Zotero/storage/MU5GNKUB/Visconti und Zubizarreta - 2018 - Handling Limited Overlap in Observational Studies .pdf:application/pdf},
}



@article{sturmer_treatment_2010,
	title = {Treatment effects in the presence of unmeasured confounding: dealing with observations in the tails of the propensity score distribution--a simulation study},
	volume = {172},
	issn = {1476-6256},
	shorttitle = {Treatment effects in the presence of unmeasured confounding},
	doi = {10.1093/aje/kwq198},
	abstract = {Frailty, a poorly measured confounder in older patients, can promote treatment in some situations and discourage it in others. This can create unmeasured confounding and lead to nonuniform treatment effects over the propensity score (PS). The authors compared bias and mean squared error for various PS implementations under PS trimming, thereby excluding persons treated contrary to prediction. Cohort studies were simulated with a binary treatment T as a function of 8 covariates X. Two of the covariates were assumed to be unmeasured strong risk factors for the outcome and present in persons treated contrary to prediction. The outcome Y was simulated as a Poisson function of T and all X's. In analyses based on measured covariates only, the range of PS's was trimmed asymmetrically according to the percentile of PS in treated patients at the lower end and in untreated patients at the upper end. PS trimming reduced bias due to unmeasured confounders and mean squared error in most scenarios assessed. Treatment effect estimates based on PS range restrictions do not correspond to a causal parameter but may be less biased by such unmeasured confounding. Increasing validity based on PS trimming may be a unique advantage of PS's over conventional outcome models.},
	language = {eng},
	number = {7},
	journal = {American Journal of Epidemiology},
	author = {Stürmer, Til and Rothman, Kenneth J. and Avorn, Jerry and Glynn, Robert J.},
	month = oct,
	year = {2010},
	pmid = {20716704},
	pmcid = {PMC3025652},
	keywords = {Bias, Calibration, Cohort Studies, Computer Simulation, Confounding Factors, Epidemiologic, Epidemiologic Methods, Humans, Models, Statistical, Regression Analysis, Research Design, Survival Rate},
	pages = {843--854},
	file = {Volltext:/Users/lennardmassmann/Zotero/storage/ZBHAQZHF/Stürmer et al. - 2010 - Treatment effects in the presence of unmeasured co.pdf:application/pdf},
}


@article{crump_dealing_2009,
	title = {Dealing with limited overlap in estimation of average treatment effects},
	volume = {96},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/asn055},
	doi = {10.1093/biomet/asn055},
	abstract = {Estimation of average treatment effects under unconfounded or ignorable treatment assignment is often hampered by lack of overlap in the covariate distributions between treatment groups. This lack of overlap can lead to imprecise estimates, and can make commonly used estimators sensitive to the choice of specification. In such cases researchers have often used ad hoc methods for trimming the sample. We develop a systematic approach to addressing lack of overlap. We characterize optimal subsamples for which the average treatment effect can be estimated most precisely. Under some conditions, the optimal selection rules depend solely on the propensity score. For a wide range of distributions, a good approximation to the optimal rule is provided by the simple rule of thumb to discard all units with estimated propensity scores outside the range [0.1,0.9].},
	number = {1},
	urldate = {2021-12-16},
	journal = {Biometrika},
	author = {Crump, Richard K. and Hotz, V. Joseph and Imbens, Guido W. and Mitnik, Oscar A.},
	month = mar,
	year = {2009},
	keywords = {\_tablet},
	pages = {187--199},
	file = {Crump et al_2009_Dealing with limited overlap in estimation of average treatment effects.pdf:/Users/lennardmassmann/Zotero/storage/VZ2RJ5FD/Crump et al_2009_Dealing with limited overlap in estimation of average treatment effects.pdf:application/pdf},
}



@article{damour_overlap_2020,
	title = {Overlap in {Observational} {Studies} with {High}-{Dimensional} {Covariates}},
	url = {http://arxiv.org/abs/1711.02582},
	abstract = {Estimating causal eﬀects under exogeneity hinges on two key assumptions: unconfoundedness and overlap. Researchers often argue that unconfoundedness is more plausible when more covariates are included in the analysis. Less discussed is the fact that covariate overlap is more diﬃcult to satisfy in this setting. In this paper, we explore the implications of overlap in observational studies with high-dimensional covariates and formalize curse-of-dimensionality argument, suggesting that these assumptions are stronger than investigators likely realize. Our key innovation is to explore how strict overlap restricts global discrepancies between the covariate distributions in the treated and control populations. Exploiting results from information theory, we derive explicit bounds on the average imbalance in covariate means under strict overlap and show that these bounds become more restrictive as the dimension grows large. We discuss how these implications interact with assumptions and procedures commonly deployed in observational causal inference, including sparsity and trimming.},
	language = {en},
	urldate = {2021-12-16},
	journal = {arXiv:1711.02582 [math, stat]},
	author = {D'Amour, Alexander and Ding, Peng and Feller, Avi and Lei, Lihua and Sekhon, Jasjeet},
	month = jan,
	year = {2020},
	note = {arXiv: 1711.02582},
	keywords = {Mathematics - Statistics Theory, \_tablet},
	file = {D'Amour et al_2020_Overlap in Observational Studies with High-Dimensional Covariates.pdf:/Users/lennardmassmann/Zotero/storage/6B29YRCJ/D'Amour et al_2020_Overlap in Observational Studies with High-Dimensional Covariates.pdf:application/pdf},
}





@article{chipman_bart_2010,
	title = {{BART}: {Bayesian} additive regression trees},
	volume = {4},
	issn = {1932-6157, 1941-7330},
	shorttitle = {{BART}},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-4/issue-1/BART-Bayesian-additive-regression-trees/10.1214/09-AOAS285.full},
	doi = {10.1214/09-AOAS285},
	abstract = {We develop a Bayesian “sum-of-trees” model where each tree is constrained by a regularization prior to be a weak learner, and fitting and inference are accomplished via an iterative Bayesian backfitting MCMC algorithm that generates samples from a posterior. Effectively, BART is a nonparametric Bayesian regression approach which uses dimensionally adaptive random basis elements. Motivated by ensemble methods in general, and boosting algorithms in particular, BART is defined by a statistical model: a prior and a likelihood. This approach enables full posterior inference including point and interval estimates of the unknown regression function as well as the marginal effects of potential predictors. By keeping track of predictor inclusion frequencies, BART can also be used for model-free variable selection. BART’s many features are illustrated with a bake-off against competing methods on 42 different data sets, with a simulation experiment and on a drug discovery classification problem.},
	number = {1},
	urldate = {2021-12-08},
	journal = {The Annals of Applied Statistics},
	author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
	month = mar,
	year = {2010},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {ensemble, Variable selection, CART, Bayesian backfitting, boosting, ‎classification‎, MCMC, Nonparametric regression, probit model, random basis, regularizatio, sum-of-trees model, weak learner, \_tablet},
	pages = {266--298},
	file = {Chipman et al_2010_BART.pdf:/Users/lennardmassmann/Zotero/storage/5I86KD7W/Chipman et al_2010_BART.pdf:application/pdf;Snapshot:/Users/lennardmassmann/Zotero/storage/TK5NNR9R/09-AOAS285.html:text/html},
}


@article{zhu_core_2021,
	title = {Core {Concepts} in {Pharmacoepidemiology}: {Violations} of the {Positivity} {Assumption} in the {Causal} {Analysis} of {Observational} {Data}: {Consequences} and {Statistical} {Approaches}},
	volume = {30},
	issn = {1053-8569},
	shorttitle = {Core {Concepts} in {Pharmacoepidemiology}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8492528/},
	doi = {10.1002/pds.5338},
	abstract = {In the causal analysis of observational data, the positivity assumption requires that all treatments of interest be observed in every patient subgroup. Violations of this assumption are indicated by nonoverlap in the data in the sense that patients with certain covariate combinations are not observed to receive a treatment of interest, which may arise from contraindications to treatment or small sample size. In this paper, we emphasize the importance and implications of this often-overlooked assumption. Further, we elaborate on the challenges nonoverlap poses to estimation and inference and discuss previously proposed methods. We distinguish between structural and practical violations and provide insight into which methods are appropriate for each. To demonstrate alternative approaches and relevant considerations (including how overlap is defined and the target population to which results may be generalized) when addressing positivity violations, we employ an electronic health record-derived data set to assess the effects of metformin on colon cancer recurrence among diabetic patients.},
	number = {11},
	urldate = {2023-10-16},
	journal = {Pharmacoepidemiology and drug safety},
	author = {Zhu, Yaqian and Hubbard, Rebecca A. and Chubak, Jessica and Roy, Jason and Mitra, Nandita},
	month = nov,
	year = {2021},
	pmid = {34375473},
	pmcid = {PMC8492528},
	pages = {1471--1485},
	file = {Zhu et al_2021_Core Concepts in Pharmacoepidemiology.pdf:/Users/lennardmassmann/Zotero/storage/2VZHVI8N/Zhu et al_2021_Core Concepts in Pharmacoepidemiology.pdf:application/pdf},
}


@article{neal_slice_2003,
	title = {Slice sampling},
	volume = {31},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-31/issue-3/Slice-sampling/10.1214/aos/1056562461.full},
	doi = {10.1214/aos/1056562461},
	abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can ample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal "slice" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such "slice sampling" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by "overrelaxation," and for multivariate slice sampling by "reflection" from the edges of the slice.},
	number = {3},
	urldate = {2023-11-17},
	journal = {The Annals of Statistics},
	author = {Neal, Radford M.},
	month = jun,
	year = {2003},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {65C05, 65C60, Adaptive methods, auxiliary variables, dynamical methods, Gibbs sampling, Markov chain Monte Carlo, Metropolis algorithm, overrelaxation},
	pages = {705--767},
	file = {Neal_2003_Slice sampling.pdf:/Users/lennardmassmann/Zotero/storage/KYLJYM8T/Neal_2003_Slice sampling.pdf:application/pdf},
}


@misc{linero_softbart_R_2022,
	title = {{SoftBart}: {Soft} {Bayesian} {Additive} {Regression} {Trees}},
	shorttitle = {{SoftBart}},
	url = {http://arxiv.org/abs/2210.16375},
	abstract = {Bayesian additive regression tree (BART) models have seen increased attention in recent years as a general-purpose nonparametric modeling technique. BART combines the ﬂexibility of modern machine learning techniques with the principled uncertainty quantiﬁcation of Bayesian inference, and it has been shown to be uniquely appropriate for addressing the high-noise problems that occur commonly in many areas of science, including medicine and the social sciences. This paper introduces the SoftBart package for ﬁtting the Soft BART algorithm of Linero and Yang (2018). In addition to improving upon the predictive performance of other BART packages, a major goal of this package has been to facilitate the inclusion of BART in larger models, making it ideal for researchers in Bayesian statistics. I show both how to use this package for standard prediction tasks and how to embed BART models in larger models; I illustrate by using SoftBart to implement a nonparametric probit regression model, a semiparametric varying coeﬃcient model, and a partial linear model.},
	language = {en},
	urldate = {2023-06-09},
	publisher = {arXiv},
	author = {Linero, Antonio R.},
	month = oct,
	year = {2022},
	note = {arXiv:2210.16375 [stat]},
	keywords = {Statistics - Methodology, Statistics - Machine Learning, Statistics - Computation},
	file = {Linero - 2022 - SoftBart Soft Bayesian Additive Regression Trees.pdf:/Users/lennardmassmann/Zotero/storage/L4GJ7CNQ/Linero - 2022 - SoftBart Soft Bayesian Additive Regression Trees.pdf:application/pdf},
}

@article{linero_bayesian_sbart_2018,
	title = {Bayesian {Regression} {Tree} {Ensembles} that {Adapt} to {Smoothness} and {Sparsity}},
	volume = {80},
	issn = {1369-7412, 1467-9868},
	url = {https://academic.oup.com/jrsssb/article/80/5/1087/7048381},
	doi = {10.1111/rssb.12293},
	abstract = {Ensembles of decision trees are a useful tool for obtaining ﬂexible estimates of regression functions. Examples of these methods include gradient-boosted decision trees, random forests and Bayesian classiﬁcation and regression trees.Two potential shortcomings of tree ensembles are their lack of smoothness and their vulnerability to the curse of dimensionality. We show that these issues can be overcome by instead considering sparsity inducing soft decision trees in which the decisions are treated as probabilistic. We implement this in the context of the Bayesian additive regression trees framework and illustrate its promising performance through testing on benchmark data sets. We provide strong theoretical support for our methodology by showing that the posterior distribution concentrates at the minimax rate (up to a logarithmic factor) for sparse functions and functions with additive structures in the high dimensional regime where the dimensionality of the covariate space is allowed to grow nearly exponentially in the sample size. Our method also adapts to the unknown smoothness and sparsity levels, and can be implemented by making minimal modiﬁcations to existing Bayesian additive regression tree algorithms.},
	language = {en},
	number = {5},
	urldate = {2023-08-17},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Linero, Antonio R. and Yang, Yun},
	month = nov,
	year = {2018},
	pages = {1087--1110},
	file = {Linero und Yang - 2018 - Bayesian Regression Tree Ensembles that Adapt to S.pdf:/Users/lennardmassmann/Zotero/storage/LMVKWLFV/Linero und Yang - 2018 - Bayesian Regression Tree Ensembles that Adapt to S.pdf:application/pdf},
}



@article{hahn_bayesian_2020,
	title = {Bayesian {Regression} {Tree} {Models} for {Causal} {Inference}: {Regularization}, {Confounding}, and {Heterogeneous} {Effects} (with {Discussion})},
	volume = {15},
	issn = {1936-0975, 1931-6690},
	shorttitle = {Bayesian {Regression} {Tree} {Models} for {Causal} {Inference}},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-15/issue-3/Bayesian-Regression-Tree-Models-for-Causal-Inference--Regularization-Confounding/10.1214/19-BA1195.full},
	doi = {10.1214/19-BA1195},
	abstract = {This paper presents a novel nonlinear regression model for estimating heterogeneous treatment effects, geared specifically towards situations with small effect sizes, heterogeneous effects, and strong confounding by observables. Standard nonlinear regression models, which may work quite well for prediction, have two notable weaknesses when used to estimate heterogeneous treatment effects. First, they can yield badly biased estimates of treatment effects when fit to data with strong confounding. The Bayesian causal forest model presented in this paper avoids this problem by directly incorporating an estimate of the propensity function in the specification of the response model, implicitly inducing a covariate-dependent prior on the regression function. Second, standard approaches to response surface modeling do not provide adequate control over the strength of regularization over effect heterogeneity. The Bayesian causal forest model permits treatment effect heterogeneity to be regularized separately from the prognostic effect of control variables, making it possible to informatively “shrink to homogeneity”. While we focus on observational data, our methods are equally useful for inferring heterogeneous treatment effects from randomized controlled experiments where careful regularization is somewhat less complicated but no less important. We illustrate these benefits via the reanalysis of an observational study assessing the causal effects of smoking on medical expenditures as well as extensive simulation studies.},
	number = {3},
	urldate = {2021-12-09},
	journal = {Bayesian Analysis},
	author = {Hahn, P. Richard and Murray, Jared S. and Carvalho, Carlos M.},
	month = sep,
	year = {2020},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {heterogeneous treatment effects, 62-07, 62F15, 62J02, Bayesian, Causal inference, machine learning, predictor-dependent priors, regression trees, regularization, shrinkage, \_tablet},
	pages = {965--1056},
	file = {Hahn et al_2020_Bayesian Regression Tree Models for Causal Inference.pdf:/Users/lennardmassmann/Zotero/storage/6SAF2ADD/Hahn et al_2020_Bayesian Regression Tree Models for Causal Inference.pdf:application/pdf;Snapshot:/Users/lennardmassmann/Zotero/storage/4KCU5T4Q/19-BA1195.html:text/html},
}



@misc{zhu_addressing_2022,
	title = {Addressing {Positivity} {Violations} in {Causal} {Effect} {Estimation} using {Gaussian} {Process} {Priors}},
	url = {http://arxiv.org/abs/2110.10266},
	abstract = {In observational studies, causal inference relies on several key identifying assumptions. One identiﬁability condition is the positivity assumption, which requires the probability of treatment be bounded away from 0 and 1. That is, for every covariate combination, it should be possible to observe both treated and control subjects, i.e., the covariate distributions should overlap between treatment arms. If the positivity assumption is violated, population-level causal inference necessarily involves some extrapolation. Ideally, a greater amount of uncertainty about the causal eﬀect estimate should be reﬂected in such situations. With that goal in mind, we construct a Gaussian process model for estimating treatment eﬀects in the presence of practical violations of positivity. Advantages of our method include minimal distributional assumptions, a cohesive model for estimating treatment eﬀects, and more uncertainty associated with areas in the covariate space where there is less overlap. We assess the performance of our approach with respect to bias and eﬃciency using simulation studies. The method is then applied to a study of critically ill female patients to examine the eﬀect of undergoing right heart catheterization.},
	language = {en},
	urldate = {2023-11-07},
	publisher = {arXiv},
	author = {Zhu, Yaqian and Mitra, Nandita and Roy, Jason},
	month = feb,
	year = {2022},
	note = {arXiv:2110.10266 [stat]},
	keywords = {Statistics - Methodology},
	file = {Zhu et al. - 2022 - Addressing Positivity Violations in Causal Effect .pdf:/Users/lennardmassmann/Zotero/storage/IULCB9MC/Zhu et al. - 2022 - Addressing Positivity Violations in Causal Effect .pdf:application/pdf},
}


@article{wang_accounting_2015,
	title = {Accounting for uncertainty in confounder and effect modifier selection when estimating average causal effects in generalized linear models},
	volume = {71},
	issn = {1541-0420},
	doi = {10.1111/biom.12315},
	abstract = {Confounder selection and adjustment are essential elements of assessing the causal effect of an exposure or treatment in observational studies. Building upon work by Wang et al. (2012, Biometrics 68, 661-671) and Lefebvre et al. (2014, Statistics in Medicine 33, 2797-2813), we propose and evaluate a Bayesian method to estimate average causal effects in studies with a large number of potential confounders, relatively few observations, likely interactions between confounders and the exposure of interest, and uncertainty on which confounders and interaction terms should be included. Our method is applicable across all exposures and outcomes that can be handled through generalized linear models. In this general setting, estimation of the average causal effect is different from estimation of the exposure coefficient in the outcome model due to noncollapsibility. We implement a Bayesian bootstrap procedure to integrate over the distribution of potential confounders and to estimate the causal effect. Our method permits estimation of both the overall population causal effect and effects in specified subpopulations, providing clear characterization of heterogeneous exposure effects that may vary considerably across different covariate profiles. Simulation studies demonstrate that the proposed method performs well in small sample size situations with 100-150 observations and 50 covariates. The method is applied to data on 15,060 US Medicare beneficiaries diagnosed with a malignant brain tumor between 2000 and 2009 to evaluate whether surgery reduces hospital readmissions within 30 days of diagnosis.},
	language = {eng},
	number = {3},
	journal = {Biometrics},
	author = {Wang, Chi and Dominici, Francesca and Parmigiani, Giovanni and Zigler, Corwin Matthew},
	month = sep,
	year = {2015},
	pmid = {25899155},
	pmcid = {PMC4575246},
	keywords = {Artifacts, Average causal effect, Bayes Theorem, Bayesian adjustment for confounding, Causality, Computer Simulation, Confounder selection, Confounding Factors, Epidemiologic, Data Interpretation, Statistical, Epidemiologic Methods, Humans, Linear Models, Outcome Assessment, Health Care, Reproducibility of Results, Sensitivity and Specificity, Treatment effect heterogeneity},
	pages = {654--665},
	file = {Akzeptierte Version:/Users/lennardmassmann/Zotero/storage/JB7SA2BE/Wang et al. - 2015 - Accounting for uncertainty in confounder and effec.pdf:application/pdf},
}


@article{westreich_invited_2010,
	title = {Invited {Commentary}: {Positivity} in {Practice}},
	volume = {171},
	issn = {0002-9262},
	shorttitle = {Invited {Commentary}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2877454/},
	doi = {10.1093/aje/kwp436},
	abstract = {Positivity, or the experimental treatment assignment assumption, requires that there be both exposed and unexposed participants at every combination of the values of the observed confounders in the population under study. Positivity is essential for inference but is often overlooked in practice by epidemiologists. This issue of the Journal includes 2 articles featuring discussions related to positivity. Here the authors define positivity, distinguish between deterministic and random positivity, and discuss the 2 relevant papers in this issue. In addition, the commentators illustrate positivity in simple 2 × 2 tables, as well as detail some ways in which epidemiologists may examine their data for nonpositivity and deal with violations of positivity in practice.},
	number = {6},
	urldate = {2023-11-28},
	journal = {American Journal of Epidemiology},
	author = {Westreich, Daniel and Cole, Stephen R.},
	month = mar,
	year = {2010},
	pmid = {20139125},
	pmcid = {PMC2877454},
	pages = {674--677},
	file = {PubMed Central Full Text PDF:/Users/lennardmassmann/Zotero/storage/FBPG3SWU/Westreich und Cole - 2010 - Invited Commentary Positivity in Practice.pdf:application/pdf},
}
