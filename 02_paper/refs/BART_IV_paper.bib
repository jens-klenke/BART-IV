
@article{bargagli_stoffi_causal_2020,
	title = {Causal tree with instrumental variable: an extension of the causal tree framework to irregular assignment mechanisms},
	volume = {9},
	issn = {2364-4168},
	shorttitle = {Causal tree with instrumental variable},
	url = {https://doi.org/10.1007/s41060-019-00187-z},
	doi = {10.1007/s41060-019-00187-z},
	abstract = {This paper provides a link between causal inference and machine learning techniques—specifically, Classification and Regression Trees—in observational studies where the receipt of the treatment is not randomized, but the assignment to the treatment can be assumed to be randomized (irregular assignment mechanism). The paper contributes to the growing applied machine learning literature on causal inference, by proposing a modified version of the Causal Tree (CT) algorithm to draw causal inference from an irregular assignment mechanism. The proposed method is developed by merging the CT approach with the instrumental variable framework to causal inference, hence the name Causal Tree with Instrumental Variable (CT-IV). An improved version, named Honest Causal Tree with Instrumental Variable (HCT-IV), able to estimate more reliably the heterogeneous causal effects, is also proposed. As compared to CT, the main strength of CT-IV and HCT-IV is that they can deal more efficiently with the heterogeneity of causal effects, as demonstrated by a series of numerical results obtained on synthetic data. Then, the proposed algorithms are used to evaluate a public policy implemented by the Tuscan Regional Administration (Italy), which aimed at easing the access to credit for small firms. In this context, HCT-IV breaks fresh ground for target-based policies, identifying interesting heterogeneous causal effects.},
	language = {en},
	number = {3},
	urldate = {2021-04-29},
	journal = {International Journal of Data Science and Analytics},
	author = {Bargagli Stoffi, Falco J. and Gnecco, Giorgio},
	month = apr,
	year = {2020},
	keywords = {\_tablet},
	pages = {315--337},
	file = {Bargagli Stoffi_Gnecco_2020_Causal tree with instrumental variable.pdf:/Users/lennardmassmann/Zotero/storage/H6M9EUHV/Bargagli Stoffi_Gnecco_2020_Causal tree with instrumental variable.pdf:application/pdf},
}

@article{athey_generalized_2019,
	title = {Generalized random forests},
	volume = {47},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1547197251},
	doi = {10.1214/18-AOS1709},
	abstract = {We propose generalized random forests, a method for nonparametric statistical estimation based on random forests (Breiman [Mach. Learn. 45 (2001) 5–32]) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: nonparametric quantile regression, conditional average partial effect estimation and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN.},
	language = {EN},
	number = {2},
	urldate = {2021-01-17},
	journal = {Annals of Statistics},
	author = {Athey, Susan and Tibshirani, Julie and Wager, Stefan},
	month = apr,
	year = {2019},
	mrnumber = {MR3909963},
	zmnumber = {07033164},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {\_tablet, Asymptotic theory, causal inference, instrumental variable},
	pages = {1148--1178},
	file = {Athey et al_2019_Generalized random forests.pdf:/Users/lennardmassmann/Zotero/storage/P7UH6PJY/Athey et al_2019_Generalized random forests.pdf:application/pdf;Snapshot:/Users/lennardmassmann/Zotero/storage/A3GUMIDH/1547197251.html:text/html},
}

@article{chipman_bart_2010,
	title = {{BART}: {Bayesian} additive regression trees},
	volume = {4},
	issn = {1932-6157, 1941-7330},
	shorttitle = {{BART}},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-4/issue-1/BART-Bayesian-additive-regression-trees/10.1214/09-AOAS285.full},
	doi = {10.1214/09-AOAS285},
	abstract = {We develop a Bayesian “sum-of-trees” model where each tree is constrained by a regularization prior to be a weak learner, and fitting and inference are accomplished via an iterative Bayesian backfitting MCMC algorithm that generates samples from a posterior. Effectively, BART is a nonparametric Bayesian regression approach which uses dimensionally adaptive random basis elements. Motivated by ensemble methods in general, and boosting algorithms in particular, BART is defined by a statistical model: a prior and a likelihood. This approach enables full posterior inference including point and interval estimates of the unknown regression function as well as the marginal effects of potential predictors. By keeping track of predictor inclusion frequencies, BART can also be used for model-free variable selection. BART’s many features are illustrated with a bake-off against competing methods on 42 different data sets, with a simulation experiment and on a drug discovery classification problem.},
	number = {1},
	urldate = {2021-12-08},
	journal = {The Annals of Applied Statistics},
	author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
	month = mar,
	year = {2010},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {ensemble, Variable selection, CART, Bayesian backfitting, boosting, ‎classification‎, MCMC, Nonparametric regression, probit model, random basis, regularizatio, sum-of-trees model, weak learner, \_tablet},
	pages = {266--298},
	file = {Chipman et al_2010_BART.pdf:/Users/lennardmassmann/Zotero/storage/5I86KD7W/Chipman et al_2010_BART.pdf:application/pdf;Snapshot:/Users/lennardmassmann/Zotero/storage/TK5NNR9R/09-AOAS285.html:text/html},
}

@article{hahn_bayesian_2020,
	title = {Bayesian {Regression} {Tree} {Models} for {Causal} {Inference}: {Regularization}, {Confounding}, and {Heterogeneous} {Effects} (with {Discussion})},
	volume = {15},
	issn = {1936-0975, 1931-6690},
	shorttitle = {Bayesian {Regression} {Tree} {Models} for {Causal} {Inference}},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-15/issue-3/Bayesian-Regression-Tree-Models-for-Causal-Inference--Regularization-Confounding/10.1214/19-BA1195.full},
	doi = {10.1214/19-BA1195},
	abstract = {This paper presents a novel nonlinear regression model for estimating heterogeneous treatment effects, geared specifically towards situations with small effect sizes, heterogeneous effects, and strong confounding by observables. Standard nonlinear regression models, which may work quite well for prediction, have two notable weaknesses when used to estimate heterogeneous treatment effects. First, they can yield badly biased estimates of treatment effects when fit to data with strong confounding. The Bayesian causal forest model presented in this paper avoids this problem by directly incorporating an estimate of the propensity function in the specification of the response model, implicitly inducing a covariate-dependent prior on the regression function. Second, standard approaches to response surface modeling do not provide adequate control over the strength of regularization over effect heterogeneity. The Bayesian causal forest model permits treatment effect heterogeneity to be regularized separately from the prognostic effect of control variables, making it possible to informatively “shrink to homogeneity”. While we focus on observational data, our methods are equally useful for inferring heterogeneous treatment effects from randomized controlled experiments where careful regularization is somewhat less complicated but no less important. We illustrate these benefits via the reanalysis of an observational study assessing the causal effects of smoking on medical expenditures as well as extensive simulation studies.},
	number = {3},
	urldate = {2021-12-09},
	journal = {Bayesian Analysis},
	author = {Hahn, P. Richard and Murray, Jared S. and Carvalho, Carlos M.},
	month = sep,
	year = {2020},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {heterogeneous treatment effects, 62-07, 62F15, 62J02, Bayesian, Causal inference, machine learning, predictor-dependent priors, regression trees, regularization, shrinkage, \_tablet},
	pages = {965--1056},
	file = {Hahn et al_2020_Bayesian Regression Tree Models for Causal Inference.pdf:/Users/lennardmassmann/Zotero/storage/6SAF2ADD/Hahn et al_2020_Bayesian Regression Tree Models for Causal Inference.pdf:application/pdf;Snapshot:/Users/lennardmassmann/Zotero/storage/4KCU5T4Q/19-BA1195.html:text/html},
}

@article{linero_bayesian_2018,
	title = {Bayesian {Regression} {Tree} {Ensembles} that {Adapt} to {Smoothness} and {Sparsity}},
	volume = {80},
	issn = {1369-7412, 1467-9868},
	url = {https://academic.oup.com/jrsssb/article/80/5/1087/7048381},
	doi = {10.1111/rssb.12293},
	abstract = {Ensembles of decision trees are a useful tool for obtaining ﬂexible estimates of regression functions. Examples of these methods include gradient-boosted decision trees, random forests and Bayesian classiﬁcation and regression trees.Two potential shortcomings of tree ensembles are their lack of smoothness and their vulnerability to the curse of dimensionality. We show that these issues can be overcome by instead considering sparsity inducing soft decision trees in which the decisions are treated as probabilistic. We implement this in the context of the Bayesian additive regression trees framework and illustrate its promising performance through testing on benchmark data sets. We provide strong theoretical support for our methodology by showing that the posterior distribution concentrates at the minimax rate (up to a logarithmic factor) for sparse functions and functions with additive structures in the high dimensional regime where the dimensionality of the covariate space is allowed to grow nearly exponentially in the sample size. Our method also adapts to the unknown smoothness and sparsity levels, and can be implemented by making minimal modiﬁcations to existing Bayesian additive regression tree algorithms.},
	language = {en},
	number = {5},
	urldate = {2023-08-17},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Linero, Antonio R. and Yang, Yun},
	month = nov,
	year = {2018},
	pages = {1087--1110},
	file = {Linero und Yang - 2018 - Bayesian Regression Tree Ensembles that Adapt to S.pdf:/Users/lennardmassmann/Zotero/storage/LMVKWLFV/Linero und Yang - 2018 - Bayesian Regression Tree Ensembles that Adapt to S.pdf:application/pdf},
}

@article{linero_bayesian_2018-1,
	title = {Bayesian {Regression} {Trees} for {High}-{Dimensional} {Prediction} and {Variable} {Selection}},
	volume = {113},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2016.1264957},
	doi = {10.1080/01621459.2016.1264957},
	abstract = {Decision tree ensembles are an extremely popular tool for obtaining high-quality predictions in nonparametric regression problems. Unmodified, however, many commonly used decision tree ensemble methods do not adapt to sparsity in the regime in which the number of predictors is larger than the number of observations. A recent stream of research concerns the construction of decision tree ensembles that are motivated by a generative probabilistic model, the most influential method being the Bayesian additive regression trees (BART) framework. In this article, we take a Bayesian point of view on this problem and show how to construct priors on decision tree ensembles that are capable of adapting to sparsity in the predictors by placing a sparsity-inducing Dirichlet hyperprior on the splitting proportions of the regression tree prior. We characterize the asymptotic distribution of the number of predictors included in the model and show how this prior can be easily incorporated into existing Markov chain Monte Carlo schemes. We demonstrate that our approach yields useful posterior inclusion probabilities for each predictor and illustrate the usefulness of our approach relative to other decision tree ensemble approaches on both simulated and real datasets. Supplementary materials for this article are available online.},
	number = {522},
	urldate = {2023-10-12},
	journal = {Journal of the American Statistical Association},
	author = {Linero, Antonio R.},
	month = apr,
	year = {2018},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2016.1264957},
	keywords = {Random forests, Variable selection, Nonparametric regression, Bayesian additive regression trees, Bayesian learning, Decision trees},
	pages = {626--636},
	file = {Linero_2018_Bayesian Regression Trees for High-Dimensional Prediction and Variable Selection.pdf:/Users/lennardmassmann/Zotero/storage/CVAXRUX7/Linero_2018_Bayesian Regression Trees for High-Dimensional Prediction and Variable Selection.pdf:application/pdf},
}

@article{bargagli-stoffi_heterogeneous_2022,
	title = {Heterogeneous causal effects with imperfect compliance: {A} {Bayesian} machine learning approach},
	volume = {16},
	issn = {1932-6157, 1941-7330},
	shorttitle = {Heterogeneous causal effects with imperfect compliance},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-16/issue-3/Heterogeneous-causal-effects-with-imperfect-compliance--A-Bayesian-machine/10.1214/21-AOAS1579.full},
	doi = {10.1214/21-AOAS1579},
	abstract = {This paper introduces an innovative Bayesian machine learning algorithm to draw interpretable inference on heterogeneous causal effects in the presence of imperfect compliance (e.g., under an irregular assignment mechanism). We show, through Monte Carlo simulations, that the proposed Bayesian Causal Forest with Instrumental Variable (BCF-IV) methodology outperforms other machine learning techniques tailored for causal inference in discovering and estimating the heterogeneous causal effects while controlling for the familywise error rate (or, less stringently, for the false discovery rate) at leaves’ level. BCF-IV sheds a light on the heterogeneity of causal effects in instrumental variable scenarios and, in turn, provides the policy-makers with a relevant tool for targeted policies. Its empirical application evaluates the effects of additional funding on students’ performances. The results indicate that BCF-IV could be used to enhance the effectiveness of school funding on students’ performance.},
	number = {3},
	urldate = {2024-07-25},
	journal = {The Annals of Applied Statistics},
	author = {Bargagli-Stoffi, Falco J. and Witte, Kristof De and Gnecco, Giorgio},
	month = sep,
	year = {2022},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {instrumental variable, Causal inference, Heterogeneous effects, Interpretable machine learning, school funding, students’ performance},
	pages = {1986--2009},
	file = {Bargagli-Stoffi et al_2022_Heterogeneous causal effects with imperfect compliance.pdf:/Users/lennardmassmann/Zotero/storage/LBR9UZJL/Bargagli-Stoffi et al_2022_Heterogeneous causal effects with imperfect compliance.pdf:application/pdf},
}

@article{caron_shrinkage_2022,
	title = {Shrinkage {Bayesian} {Causal} {Forests} for {Heterogeneous} {Treatment} {Effects} {Estimation}},
	volume = {31},
	issn = {1061-8600},
	url = {https://doi.org/10.1080/10618600.2022.2067549},
	doi = {10.1080/10618600.2022.2067549},
	abstract = {This article develops a sparsity-inducing version of Bayesian Causal Forests, a recently proposed nonparametric causal regression model that employs Bayesian Additive Regression Trees and is specifically designed to estimate heterogeneous treatment effects using observational data. The sparsity-inducing component we introduce is motivated by empirical studies where not all the available covariates are relevant, leading to different degrees of sparsity underlying the surfaces of interest in the estimation of individual treatment effects. The extended version presented in this work, which we name Shrinkage Bayesian Causal Forest, is equipped with an additional pair of priors allowing the model to adjust the weight of each covariate through the corresponding number of splits in the tree ensemble. These priors improve the model’s adaptability to sparse data generating processes and allow to perform fully Bayesian feature shrinkage in a framework for treatment effects estimation, and thus to uncover the moderating factors driving heterogeneity. In addition, the method allows prior knowledge about the relevant confounding covariates and the relative magnitude of their impact on the outcome to be incorporated in the model. We illustrate the performance of our method in simulated studies, in comparison to Bayesian Causal Forest and other state-of-the-art models, to demonstrate how it scales up with an increasing number of covariates and how it handles strongly confounded scenarios. Finally, we also provide an example of application using real-world data. Supplementary materials for this article are available online.},
	number = {4},
	urldate = {2024-07-25},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Caron, Alberto and Baio, Gianluca and Manolopoulou, Ioanna},
	month = oct,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10618600.2022.2067549},
	keywords = {Causal inference, Machine learning, Bayesian nonparametrics, Heterogeneous treatment effects, Observational studies, Tree ensembles},
	pages = {1202--1214},
	file = {Caron et al_2022_Shrinkage Bayesian Causal Forests for Heterogeneous Treatment Effects Estimation.pdf:/Users/lennardmassmann/Zotero/storage/A4GKK75T/Caron et al_2022_Shrinkage Bayesian Causal Forests for Heterogeneous Treatment Effects Estimation.pdf:application/pdf},
}

@book{breiman_classification_2017,
	address = {New York},
	title = {Classification and {Regression} {Trees}},
	isbn = {978-1-315-13947-0},
	abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
	publisher = {Chapman and Hall/CRC},
	author = {Breiman, Leo and Friedman, Jerome and Olshen, R. A. and Stone, Charles J.},
	month = oct,
	year = {2017},
	doi = {10.1201/9781315139470},
}

@inproceedings{hartford_deep_2017,
	title = {Deep {IV}: {A} {Flexible} {Approach} for {Counterfactual} {Prediction}},
	shorttitle = {Deep {IV}},
	url = {https://proceedings.mlr.press/v70/hartford17a.html},
	abstract = {Counterfactual prediction requires understanding causal relationships between so-called treatment and outcome variables. This paper provides a recipe for augmenting deep learning methods to accurately characterize such relationships in the presence of instrument variables (IVs) – sources of treatment randomization that are conditionally independent from the outcomes. Our IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework allows us to take advantage of off-the-shelf supervised learning techniques to estimate causal effects by adapting the loss function. Experiments show that it outperforms existing machine learning approaches.},
	language = {en},
	urldate = {2024-08-22},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hartford, Jason and Lewis, Greg and Leyton-Brown, Kevin and Taddy, Matt},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1414--1423},
	file = {Hartford et al_2017_Deep IV.pdf:/Users/lennardmassmann/Zotero/storage/LR88UMWA/Hartford et al_2017_Deep IV.pdf:application/pdf;Supplementary PDF:/Users/lennardmassmann/Zotero/storage/ER7GY66Z/Hartford et al. - 2017 - Deep IV A Flexible Approach for Counterfactual Pr.pdf:application/pdf},
}
