\chapter{Bayesian Causal Forests and
Instrumental Variables}

This paper proposes an algorithm for the estimation of cCACE for sparse data scenarios. 
More precisely, we propose an extension of the BCF-IV algorithm in \cite{bargagli-stoffi_heterogeneous_2022} 
to handle scenarios with many irrelevant covariates in the dataset.
Section \ref{sec:BCF-IV} describes the original BCF-IV algorithm while section
\ref{sec:SBCF-IV} explains the proposed extension based on the Shrinkage Bayesian Causal Forest. As pointed out in the literature review of this paper, current ensemble methods that operate under an irregular assignment mechanism with imperfect compliance face some difficulties. Algorithms like Deep IV \cite{hartford_deep_2017} and the Generalized Random Forest \cite{athey_generalized_2019} provide precise cCACE estimates but are rather uninformative about relevant covariates, or subsets of possibly many covariates, that drive heterogeneity in cCACE. Tree-based methods like the Causal Tree with IV \cite{bargagli_stoffi_causal_2020} propose to estimate treatment effects under imperfect compliance and the existence of a suitable IV while retainig interpretability. However, although the single tree structure enables interpretability it also lacks of stability and replicability. \cite{bargagli-stoffi_heterogeneous_2022} argue to overcome those shortcomings using the BCF-IV algorithm as outlined in the following Section \ref{sec:BCF-IV}.   


\section{BCF-IV}
\label{sec:BCF-IV}

The main steps of the BCF-IV algorithm are outlined in Algorithm \ref{algo:BCF-IV}. Details of these three steps of honest sample splitting, discovery of treatment effect heterogeneity and inference of treatment effects are discussed in Sections \ref{honest_splitting}, \ref{discovery} and \ref{inference}.

\subsection{Honest sample splitting}
\label{honest_splitting}
The first step concerns honest sample splitting. This step enables a data-driven discovery of heterogeneous subgroups such that there is no need to specify those subgroups beforehand. Defining subgroups before estimating treatment effects based on relevant data of the studied population is a challenging task. It requires deep knowledge about the intricacies of the treatment effect at hand and may be prone to overlook relevant subgroups. Honest sample splitting as proposed in \cite{Athey_imbens_2016} is a remedy for those issues by making distinctions between model selection and treatment effect inference.    

\begin{algorithm}[H]
    \footnotesize
    \DontPrintSemicolon
    \SetAlgoLined
    \LinesNotNumbered
    \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
    %\SetKwBlock{BARTBMA}{BART-BMA(Y,x, hyperparams)}{} 
    \Input{$N$ units $i$ $(X_i, Z_i, W_i, Y_i)$, with feature vector $X_i$, treatment assignment (instrumental variable) $Z_i$, treatment receipt $W_i$, observed response $Y_i$} 
    \Output{A tree structure discovering the heterogeneity in the causal effects and estimates of the Complier Average Causal Effects (CACE) within its leaves.}
    \BlankLine
    \textbf{1. The Honest Splitting Step:} \\
    \begin{itemize}
        \item Randomly split the total sample into a discovery subsample ($I_{\text{dis}}$) and an inference subsample ($I_{\text{inf}}$).
    \end{itemize}
    \BlankLine
    \textbf{2. The Discovery Step} (performed on $I_{\text{dis}}$): \\
    Estimation of the Conditional CACE:
    \begin{itemize}
        \item (a) Estimate the conditional Intention-To-Treat: $\widehat{\text{ITT}}(x)$.
        \item (b) Estimate the conditional proportion of compliers: $\widehat{\pi}_C(x)$.
        \item (c) Estimate the conditional CACE, $\widehat{\tau}_{\text{CACE}}(x)$, using the estimated values from (a) and (b).
    \end{itemize}
    Heterogeneous subpopulations discovery:
    \begin{itemize}
        \item (d) Discover the heterogeneous effects by fitting a decision tree using the data $(\widehat{\tau}_{\text{CACE}}(x), X_i)$.
    \end{itemize}
    \BlankLine
    \textbf{3. The Inference Step} (performed on $I_{\text{inf}}$): \\
    \begin{itemize}
        \item (a) Estimate the $\widehat{\tau}_{\text{CACE}}(x)$ for all discovered subpopulations (i.e., nodes and leaves) in the tree discovered in Step 3(d).
        \item (b) Perform multiple hypothesis tests and adjust p-values to control for the familywise error rate or, less stringently, the false discovery rate. 
        \item (c) Run weak-instrument tests within every node and discard nodes where weak-instrument issues are detected.
    \end{itemize}
    \caption{Bayesian Causal Forest with Instrumental Variable (BCF-IV)}
    \label{algo:BCF-IV}
\end{algorithm}


\subsection{Discovery of heterogeneous subgroups}
\label{discovery}

To estimate the conditional CACE given in Definition \ref{defn:cCACE_estimator}, we need some functional expression for the conditional expected value for the outcome, $Y_i$, as well as for the treatment indicator, $W_i$.  
The Bayesian Causal Forest (BCF) algorithm is proposed in \cite{hahn_bayesian_2020} to use the Bayesian Additive Regression Trees (BART) algorithm \cite{chipman_bart_2010} to estimate the CATE of Definition \ref{defn:CATE} in a regular assignment mechanism. BART is related to the CART algorithm of \cite{Breiman1984} which constructs binary trees by recursively partitioning the covariate space to produce accurate predictions. 
BART rests on a complete Bayesian probability model by using different regularizing prior distributions such that the overall model fit dominates fits of single trees. Distinct prior distributions are used for the complexity of the tree structure, data shrinkage within the nodes and the variance of the error term. The same idea is now transferred to the setup of an irregular assignment mechanism.
We start with modeling the numerator of Definition \ref{defn:cCACE_estimator} and restrict our dataset to observations within $\mathcal{I}_{disc}$.
Let the outcome variable $Y_i$ be modelled semi-parametrically as 
\begin{align*}
    Y_i = f(Z_i, X_i) + \varepsilon_i 
\end{align*}
with $\varepsilon_i \sim \left(0, \sigma_{\varepsilon}^2\right)$. The conditional expected value of the outcome be defined similar to \cite{hahn_bayesian_2020} as 

\begin{align*}
    \mathbb{E}\left[Y_i | Z_i = z, X_i=x \right] = \mu\left(\pi(x), x\right) + ITT_y(x)z. 
\end{align*}
Therefore, the conditional expectation is mainly governed by two additive functions. 
The first additive term $\mu\left(\pi(x), x\right)$ incorporates the IV's propensity score $\pi(x)=\mathbb{E}[Z_i=1 | X_i=x]$ and accounts for the direct, treatment-independent influence of the control variables on the outcome variable. 
The usage of the propensity score $\pi(x)$ prevents targeted selection and regularization-induced confounding. 
The second additive component $ITT_y(x)$ accounts for the direct, possibly heterogeneous, intention-to-treat effect. 
While the first term follows the standard prior specifications as in \cite{chipman_bart_2010}, the second term uses alternative tree depth penalty parameters $(\eta = 3, \beta=0.25)$ that encourage rather simplistic trees \cite{hahn_bayesian_2020}.
Consequently, we can get estimates of $\widehat{ITT}_y(x)$ as required in the discovery step 2(a) of Algorithm \ref{algo:BCF-IV}.
Analogously, we can model the denominator of Definition \ref{defn:cCACE_estimator} by still restricting our dataset to observations within $\mathcal{I}_{disc}$ and using
\begin{align*}
    W_i = f(Z_i, X_i) + \varphi_i 
\end{align*}
with $\varphi_i \sim \left(0, \sigma_{\varphi}^2\right)$. The conditional expected value of the treatment indicator is now defined as       
\begin{align*}
    \mathbb{E}\left[W_i | Z_i = z, X_i=x \right] = \delta(z, x)
\end{align*}
This conditional expectation is governed by a standard BART prior such that the denominator of Definition \ref{defn:cCACE_estimator} is estimated using BART in the sense of \cite{Hill_2011}. This gives us estimates $\widehat{\pi}_{C}(x)$ as required in discovery step 2 (b) of Algorithm \ref{algo:BCF-IV} such that, together with $\widehat{ITT}_y(x)$, we can compute $\widehat\tau_{CACE}(x)$ straightforward.
Finally, a CART algorithm is used on the estimates $\widehat\tau_{CACE}(x)$ to discover heterogeneity in CACE while retaining interpretability by providing transperancy on which variables have been used to construct the relevant subgroups. 

\subsection{Inference of conditional CACE}
\label{inference}
Finally, Definition \ref{defn:cCACE_estimator} is used in all subgroups independently using the tree structure learned in Subsection \ref{discovery} with unseen data from $\mathcal{I}_{inf}$ to infer conditional CACE by exploiting honest sample splitting outlined in Subsection \ref{honest_splitting}.  




\section{Shrinkage BCF-IV}
\label{sec:SBCF-IV}



